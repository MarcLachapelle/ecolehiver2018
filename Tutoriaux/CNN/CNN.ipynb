{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fNlYWG6z9GKT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Install dependencies\n",
        "use_conda=`which conda`\n",
        "if [ $use_conda ] ; then\n",
        "  # Jupyter\n",
        "  conda install --quiet --yes Pillow==4.0.0 matplotlib unzip\n",
        "else\n",
        "  # Colab\n",
        "  pip3 --quiet install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl torchvision Pillow==4.0.0 matplotlib\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FNomSpLO9LeH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Le jeu de données MNIST\n",
        "MNIST est le **jeu de données de référence de classification** utilisé en **vision par ordinateur**. Il est hébergé sur le <a href=\"http://yann.lecun.com/exdb/mnist/\">le site de Yann LeCun</a>. Il se compose d'**images de chiffres manuscripts**. Quelques exemples sont données ci-dessous :\n",
        "\n",
        "<img src=\"https://github.com/mila-udem/ecolehiver2018/blob/master/Tutoriaux/CNN/figures/mnist.png?raw=true\">\n",
        "\n",
        "Il inclut également des **étiquettes de classes pour chaque image**, indiquant à quel chiffre elle correspond. Par exemple, les étiquettes des images ci-dessus sont 5, 0, 4 et 1.\n",
        "\n",
        "Il se compose de **60 000 exemples d'entraînement** et de **10 000 exemples de test**. Les images sont toutes de la même taille (**28x28 pixels**). Chaque pixel est représenté par un chiffre entre 0 et 255 indiquant une nuance de gris. En fonction des modèles que nous allons tester les images seront utilisées telles quelles ou bien aplaties."
      ]
    },
    {
      "metadata": {
        "id": "F1U2sDz9ufy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Télécharger les données et créer le chargeur de données\n",
        "### Boîte à outils\n",
        "**Rappel:** Dans PyTorch, il existe des fonctions pour charger, mélanger et augmenter les données. \n",
        "\n",
        "Une façon simple de charger les données dans PyTorch est : \n",
        "<ul>\n",
        "<li>D'utiliser une classe enfant de <a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\">`torch.utils.data.Dataset`</a> où les méthodes `__getitem__` et `__len__` sont à compléter.</li>\n",
        "<li>D'utiliser la classe <a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\">`torch.utils.data.DataLoader`</a> pour lire et mettre en mémoire les données.</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "Par chance, dans pytorch, il existe déjà une classe enfant de Dataset pour utiliser MNIST : <a href=\"http://pytorch.org/docs/master/torchvision/datasets.html#mnist\">`torchvision.datasets.MNIST`</a>.\n",
        "\n",
        "<a href=\"http://pytorch.org/docs/master/torchvision/datasets.html\">D'autres jeux de données sont aussi disponibles.</a>\n",
        "\n",
        "**Remarque:** <a href=\"http://pytorch.org/docs/master/tensors.html#torch.Tensor.view\">`torch.Tensor.view()`</a> renvoie un nouveau tenseur avec les mêmes données que le tenseur d'origine mais avec une taille différente. Cela peut donc être utilisé pour aplatir une image, par exemple."
      ]
    },
    {
      "metadata": {
        "id": "n2iC4F8H8bsx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import sampler, DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "manualSeed = 1234\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# Fixing random seed\n",
        "random.seed(manualSeed)\n",
        "np.random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "if use_gpu:\n",
        "   torch.cuda.manual_seed_all(manualSeed)\n",
        "\n",
        "class ChunkSampler(sampler.Sampler):\n",
        "    \"\"\"Samples elements sequentially from some offset.\n",
        "    From: https://github.com/pytorch/vision/issues/168\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_samples: int\n",
        "      # of desired datapoints\n",
        "    start: int\n",
        "      Offset where we should start selecting from\n",
        "    \"\"\"\n",
        "    def __init__(self, num_samples, start=0):\n",
        "        self.num_samples = num_samples\n",
        "        self.start = start\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(range(self.start, self.start + self.num_samples))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "train_dataset = MNIST(root='../data', \n",
        "                      train=True, \n",
        "                      transform=transforms.ToTensor(),  \n",
        "                      download=True)\n",
        "\n",
        "test_dataset = MNIST(root='../data', \n",
        "                     train=False, \n",
        "                     transform=transforms.ToTensor())\n",
        "\n",
        "train_dataset_sizes = len(train_dataset)\n",
        "num_train_samples = int(0.8 * train_dataset_sizes)\n",
        "num_valid_samples = train_dataset_sizes - num_train_samples\n",
        "num_test_samples = len(test_dataset)\n",
        "\n",
        "print('# of train examples: {}'.format(num_train_samples))\n",
        "print('# of valid examples: {}'.format(num_valid_samples))\n",
        "print('# of test examples: {}'.format(num_test_samples))\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          sampler=ChunkSampler(num_train_samples, 0),\n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False)\n",
        "\n",
        "valid_loader = DataLoader(dataset=train_dataset,\n",
        "                          sampler=ChunkSampler(\n",
        "                              num_valid_samples, num_train_samples),\n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, \n",
        "                         batch_size=batch_size, \n",
        "                         shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gRV8zZbHV6zN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualisons les données d'entraînement !"
      ]
    },
    {
      "metadata": {
        "id": "TB57DZYzV9Oz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "inputs, classes = next(iter(train_loader))\n",
        "\n",
        "print('Inputs size: {}'.format(inputs.size()))\n",
        "print('Classes size: {}'.format(classes.size()))\n",
        "\n",
        "# First image of the batch\n",
        "img1 = 255 - inputs[0] * 255\n",
        "\n",
        "# Plot the image\n",
        "print('\\n\\nDisplay the first image:')\n",
        "img1 = img1.numpy()[0, :, :]\n",
        "plt.imshow(img1, cmap='gray', vmin=0, vmax=255)\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T_XSUTbG0UvX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CPU ou GPU\n",
        "**Rappel:** <a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> est une librairie qui permet d'utiliser des GPUs pour effectuer les calculs sur des tenseurs. La librairie inclus des tenseurs de type CUDA qui ont les mêmes fonctions que les tenseurs réguliers mais qui utilisent des GPUs pour leurs calculs, au lieu d'un CPU. <a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> retourne un booléen indiquant si CUDA est présentement disponible. Pour passer d'un tenseur de type CPU à un tenseur de type GPU, il suffit de lui ajouter `.cuda()`.\n"
      ]
    },
    {
      "metadata": {
        "id": "LxnZv9g_0RQK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2LZ9sxy8bs3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Le perceptron multi-couche\n",
        "Un perceptron multi-couche est un réseau feed-forward simple. Il prend en entrée les images, les transforme à travers une série de couches cachées et finalement donne une sortie. Cette sortie correspond à la probabilité d'appartenance à l'une ou l'autre des classes de la cible.\n",
        "\n",
        "Par exemple, si on regarde un perceptron multi-couche qui classifie des images de chiffres du jeu de données MNIST :\n",
        "\n",
        "<img src=\"https://github.com/mila-udem/ecolehiver2018/blob/master/Tutoriaux/CNN/figures/mlp.png?raw=true\">\n",
        "\n",
        "La procédure d'apprentissage typique pour ce modèle consiste en :\n",
        "<ul>\n",
        "<li>Définir l'architecture du réseau. Cela définira les paramètres (poids et biais) du réseau.</li>\n",
        "<li>Définir la fonction de coût et l'optimiseur.</li>\n",
        "<li>Entraîner le réseau.</li>\n",
        "<li>Tester le réseau.</li>\n",
        "</ul>\n",
        "\n",
        "Notez que cette procèdure est valable pour l'entraînement de tous type de réseau de neurones profonds.\n",
        "\n",
        "### Boîte à outils\n",
        "\n",
        "Rappelons qu'un réseau de neurones profond peut être construit en utilisant la librairie <a href=\"http://pytorch.org/docs/master/nn.html\">`torch.nn`</a>. `nn` travaille avec <a href=\"http://pytorch.org/docs/master/autograd.html\">`torch.autograd`</a> pour définir et différencier les modèles."
      ]
    },
    {
      "metadata": {
        "id": "CUhdXQNdup2c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Définir l'architecture du réseau\n",
        "### Boîte à outils\n",
        "\n",
        "Pour définir l'architecture du réseau en pytorch il faut créer une classe enfant de la classe parent <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Module\">`torch.nn.Module`</a> où les méthodes suivantes sont à compléter :\n",
        "<ul>\n",
        "<li>La méthode `__init__` qui définit les couches. </li>\n",
        "<li>La méthode `forward(input)` qui retourne l'`output`.</li>\n",
        "</ul>\n",
        "\n",
        "Pour construire les couches de l'`__init__` du perceptron multi-couche, les classes suivantes peuvent être utilisées :\n",
        "<ul>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> qui applique une transformation linéaire aux données d'entrée : y = Ax + b.</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.ReLU\">`torch.nn.Relu()`</a> qui applique la fonction Relu éléments par éléments : Relu(x) = max(0, x).</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Softmax\">`torch.nn.Softmax(dim)`</a> qui applique la fonction Sofmax à un tenseur d'entrée à n-dimension en le normalisant de tel sorte que les éléments de tenseur de sortie à n-dimension soient dans l'intervalle [0, 1] et somment à 1.</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\">`torch.nn.sequential`</a>  qui est un conteneur séquentiel dans lequel les modules sont ajoutés dans l'ordre dans lequel ils sont passés au constructeur.</li>\n",
        "</ul>\n",
        "\n",
        "Dans `forward(input)` on applique aux données d'entrée les différentes couches définies dans `__init__` les unes après les autres.\n",
        "\n",
        "Enfin, `model.cuda()` permet de passer le modèle sur GPU.\n",
        "\n",
        "## Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "e-3XHqgI8bs4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.hidden_layer = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU())\n",
        "        \n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_size, num_classes))\n",
        "    \n",
        "    def forward(self, x):        \n",
        "        \n",
        "        out = self.hidden_layer(x)\n",
        "        \n",
        "        out = self.output_layer(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "\n",
        "if use_gpu:\n",
        "  # switch model to GPU\n",
        "  model.cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PycCW5MSUXFm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# Save the initial weights of model\n",
        "init_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P-t4PftT8bs7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Définir la fonction de coût et l'optimiseur\n",
        "### Boîte à outils\n",
        "De nombreuses fonctions de coût et optimiseurs sont disponibles dans Pytorch. \n",
        "\n",
        "Rappelons qu'une fonction de coût $J(\\theta)$ prend en entrée le couple (prédiction, cible) et calcule une valeur qui estime la distance entre la prédiction et la cible. L'optimiseur dans le cas de la descente de gradient stochastique, ou Stochastic Gradient Descent (SGD), minimise la fonction de coût $J(\\theta)$ paramétrisée par les poids du modèle $\\theta \\in \\mathbb{R}^d$ en mettant à jour les poids itérativement suivant cette règle simple : `poids = poids - pas_d_apprentissage * gradient`.\n",
        "\n",
        "Un choix commun pour un problème de classification (notre cas) est d'utiliser les classes suivantes :\n",
        "<ul>\n",
        "<li>**Fonction de coût :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. L'entropie croisée est souvent utilisée en optimisation. Elle permet de comparer une distribution $p$ avec une distibution de référence $t$. Elle est minimum lorsque $t=p$. Sa formule pour la calculer entre la prédiction et la cible est : $-\\sum_j t_{ij} \\log(p_{ij})$ où $p$ est la prédiction, $t$ la cible, $i$ les exemples et $j$ les classes de la cible.</li>\n",
        "<li>**Optimiseur :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> qui est une implémentation de SGD.</li>\n",
        "</ul>\n",
        "\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "Rg7HNpSy8bs8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1DUif-H8bs_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entraîner le réseau\n",
        "### Boîte à outils\n",
        "En général, l'entraînement d'un réseau se fait en itérant sur plusieurs époques (une époque correspond à une passe sur l'intégralité du jeu de données d'entraînement). Sur une époque on va recevoir une série de batches fournies par l'itérateur. Pour chaque batch, on fait les opérations suivantes:\n",
        "<ul>\n",
        "<li>`optimizer.zero_grad()` : on efface les gradients encore stockés par le réseau issus de la passe précédente.</li>\n",
        "<li>`loss.backward()` : on calcule automatiquement la dérivée du coût et on propage l'erreur dans le graphe par rétro-propagation.</li>\n",
        "<li>`optimizer.step()` : on effectue une étape de descente de gradient. Dans le cas de SGD, c'est une descente de gradient classique avec les gradients calculés précédemment : `poids = poids - pas_d_apprentissage * gradient`.</li>\n",
        "</ul>\n",
        "\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "JVh2Pbq98btA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "from torch.autograd import Variable\n",
        "\n",
        "model.load_state_dict(init_model_wts)\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.data[0]\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Statistics\n",
        "        valid_loss += loss.data[0]\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AM0yhihy_rjR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualisons les courbes d'entraînement !"
      ]
    },
    {
      "metadata": {
        "id": "WHUMeoJy_qNX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save history for later\n",
        "mlp_train_loss_history = train_loss_history\n",
        "mlp_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, mlp_train_loss_history, label='train')\n",
        "plt.plot(x, mlp_valid_loss_history, label='valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4cLnlFhW8btC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tester le réseau\n",
        "### Boîte à outils\n",
        "On évalue ensuite le réseau sur l'ensemble du jeu de données de test.\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "OX6_p5wx_tYv",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test data\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    if use_gpu:\n",
        "      # switch tensor type to GPU\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    # Flatten the images\n",
        "    images = images.view(-1, 28*28)\n",
        "    \n",
        "    # Convert torch tensor to Variable\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    # Forward\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    correct += torch.sum(predicted == labels.data)\n",
        "\n",
        "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hp1YgKZb8btG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LeNet\n",
        "LeNet est un réseau à convolution simple pour la classification. Il en existe plusieurs versions. Il est préférable d'utiliser un réseau à convolution pour de la classification d'images car ce type de réseau prend en compte la structure de l'image et à taille de réseau équivalent a un nombre de paramètres plus faible.\n",
        "\n",
        "Par exemple, si on prend l'exemple de LeNet 5 pour classifier les images de chiffres du jeu de données MNIST :\n",
        "\n",
        "<img src=\"https://github.com/mila-udem/ecolehiver2018/blob/master/Tutoriaux/CNN/figures/lenet5.png?raw=true\">\n",
        "\n",
        "La procédure d'apprentissage typique pour ce modèle est la même que pour le perceptron multi-couche et consiste en :\n",
        "<ul>\n",
        "<li>Définir l'architecture du réseau. Cela définira les paramètres (poids et biais) du réseau.</li>\n",
        "<li>Définir la fonction de coût et l'optimiseur.</li>\n",
        "<li>Entraîner le réseau.</li>\n",
        "<li>Tester le réseau.</li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "id": "owIcd9VguyAe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Définir l'architecture du réseau\n",
        "### Boîte à outils\n",
        "**Rappel :** Pour définir l'architecture du réseau en pytorch il faut créer une classe enfant de la classe parent <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Module\">`torch.nn.Module`</a> où les méthodes suivantes sont à compléter :\n",
        "<ul>\n",
        "<li>La méthode `__init__` qui définit les couches. </li>\n",
        "<li>La méthode `forward(input)` qui retourne l'`output`.</li>\n",
        "</ul>\n",
        "\n",
        "Pour construire les couches de l'`__init__` de LeNet 5, les classes suivantes peuvent être utilisées :\n",
        "<ul>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Conv2d\">`torch.nn.Conv2d(in_channels, out_channels, kernel_size)`</a> qui applique une convolution 2D sur un signal d'entrée composé de plusieurs canaux d'entrée.</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.MaxPool2d\">`torch.nn.MaxPool2d(kernel_size)`</a> qui applique du max pooling 2D sur un signal d'entrée composé de plusieurs canaux d'entrée.</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> qui applique une transformation linéaire aux données d'entrée : y = Ax + b.</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.ReLU\">`torch.nn.Relu()`</a> qui applique la fonction Relu éléments par éléments : Relu(x) = max(0, x).</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Softmax\">`torch.nn.Softmax(dim)`</a> qui applique la fonction Sofmax à un tenseur d'entrée à n-dimension en le normalisant de tel sorte que les éléments de tenseur de sortie à n-dimension soient dans l'intervalle [0, 1] et somment à 1.</li>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\">`torch.nn.sequential`</a>  qui est un conteneur séquentiel dans lequel les modules sont ajoutés dans l'ordre dans lequel ils sont passés au constructeur.</li>\n",
        "</ul>\n",
        "\n",
        "Dans `forward(input)` on applique aux données d'entrée les différentes couches définies dans `__init__` les unes après les autres.\n",
        "\n",
        "Enfin, `torch.cuda()` permet de passer le modèle sur GPU.\n",
        "\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "sR4OQa-4gBAQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        \n",
        "        self.fc = nn.Linear(7*7*32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.block1(x)\n",
        "\n",
        "        out = self.block2(out)\n",
        "        \n",
        "        # Flatten the output of block2\n",
        "        out = out.view(out.size(0), -1)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out\n",
        "        \n",
        "model = LeNet5()\n",
        "\n",
        "if use_gpu:\n",
        "  # switch model to GPU\n",
        "  model.cuda()\n",
        "  \n",
        "print(model)\n",
        "\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tRo03AR2PFPE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ici, on observe 28 938 paramètres pour LeNet5 contre 648 010 paramètres pour le MLP à deux couches cachées. On a donc une réduction significative du nombre de paramètres entre LeNet5 et le MLP précédent."
      ]
    },
    {
      "metadata": {
        "id": "WuPJFPoDQYLq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save the initial weights of model\n",
        "init_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y9H0ssbh3V1j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Définir la fonction de coût et l'optimiseur\n",
        "### Boîte à outils\n",
        "**Rappel : ** un choix commun pour un problème de classification (notre cas) est d'utiliser les classes suivantes :\n",
        "<ul>\n",
        "<li>**Fonction de coût :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. L'entropie croisée est souvent utilisée en optimisation. Elle permet de comparer une distribution $p$ avec une distibution de référence $t$. Elle est minimum lorsque $t=p$. Sa formule pour la calculer entre la prédiction et la cible est : $-\\sum_j t_{ij} \\log(p_{ij})$ où $p$ est la prédiction, $t$ la cible, $i$ les exemples et $j$ les classes de la cible.</li>\n",
        "<li>**Optimiseur :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a> qui est une implémentation de SGD.</li>\n",
        "</ul>\n",
        "\n",
        "### Implémentation\n"
      ]
    },
    {
      "metadata": {
        "id": "uE3Cfrd-hJ0e",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYEtUb4s3rab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entraîner le réseau\n",
        "### Boîte à outils\n",
        "**Rappel :** en général, l'entraînement d'un réseau se fait en itérant sur plusieurs époques (une époque correspond à une passe sur l'intégralité du jeu de données d'entraînement). Sur une époque on va recevoir une série de batches fournies par l'itérateur. Pour chaque batch, on fait les opérations suivantes:\n",
        "<ul>\n",
        "<li>`optimizer.zero_grad()` : on efface les gradients encore stockés par le réseau issus de la passe précédente.</li>\n",
        "<li>`loss.backward()` : on calcule automatiquement la dérivée du coût et on propage l'erreur dans le graphe par rétro-propagation.</li>\n",
        "<li>`optimizer.step()` : on effectue une étape de descente de gradient. Dans le cas de SGD, c'est une descente de gradient classique avec les gradients calculés précédemment : `poids = poids - pas_d_apprentissage * gradient`.</li>\n",
        "</ul>\n",
        "\n",
        "### Implémentation\n",
        "C'est simple, compléter les trous...\n",
        "\n",
        "Bonne chance !"
      ]
    },
    {
      "metadata": {
        "id": "elxupovwhRSk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_state_dict(init_model_wts)\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.data[0]\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Statistics\n",
        "        valid_loss += loss.data[0]\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3fozzXmdRGTs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualisons les courbes d'entraînement !"
      ]
    },
    {
      "metadata": {
        "id": "TUGbFeg5RHFZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save history for later\n",
        "lenet5_train_loss_history = train_loss_history\n",
        "lenet5_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, lenet5_train_loss_history, label='train')\n",
        "plt.plot(x, lenet5_valid_loss_history, label='valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H6kWtULrSDXL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On peut superposer les courbes d'entraînement et de validation de LeNet5 et du MLP :"
      ]
    },
    {
      "metadata": {
        "id": "rE1qsmvaSTjH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, mlp_train_loss_history, label='MLP train')\n",
        "plt.plot(x, mlp_valid_loss_history, label='MLP valid')\n",
        "plt.plot(x, lenet5_train_loss_history, label='LeNet5 train')\n",
        "plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tq79RTld3xyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tester le réseau\n",
        "### Boîte à outils\n",
        "**Rappel :** on évalue ensuite le réseau sur l'ensemble du jeu de données de test.\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "da47-MilhpN7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over data.\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    if use_gpu:\n",
        "      # switch tensor type to GPU\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    # No need to flatten the images here !\n",
        "    \n",
        "    # Convert torch tensor to Variable\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    # Forward\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    correct += torch.sum(predicted == labels.data)\n",
        "\n",
        "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "voN-_iO_RQ8A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On obtient de meilleurs résultats après 10 époques !\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OFj_W39u5voa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Méthodes pratique pour améliorer l'entraînement "
      ]
    },
    {
      "metadata": {
        "id": "chov57bzu76J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Batch normalisation\n",
        "La batch normalisation est une astuce qui permet, en pratique, au modèle d'apprendre plus vite. Elle agit comme régularisateur en normalisant les entrées par batch, de manière différentiable.\n",
        "\n",
        "### Boîte à outils\n",
        "Pour ajouter la batch normalisation dans LeNet5, il suffit de l'ajouter parmis les couches de l'`__init__`. La classe suivante peut être utilisée:\n",
        "<ul>\n",
        "<li><a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.BatchNorm2d\">`nn.BatchNorm2d(num_features)`</a> : permet d'ajouter de la batch normalisation à une entrée à 4 dimensions présentée sous la forme d'un tenseur à 3 dimensions.</li>\n",
        "</ul>\n",
        "\n",
        "### Implémentation\n",
        "Ci-dessous vous pouvez remarquer que la classe LeNet5 a été modifiée pour y ajouter la batch normalisation."
      ]
    },
    {
      "metadata": {
        "id": "6pi_mhvg8E4E",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        \n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2))\n",
        "        \n",
        "        self.fc = nn.Linear(7*7*32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.block1(x)\n",
        "\n",
        "        out = self.block2(out)\n",
        "        \n",
        "        # Flatten the output of block2\n",
        "        out = out.view(out.size(0), -1)\n",
        "        \n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out\n",
        "        \n",
        "model = LeNet5()\n",
        "\n",
        "if use_gpu:\n",
        "  # switch model to GPU\n",
        "  model.cuda()\n",
        "  \n",
        "print(model)\n",
        "\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "271HrQBNccH1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ici, on observe 29 034 paramètres pour LeNet5 avec batch normalisation contre 28 938 paramètres pour LeNet5 sans batch normalisation."
      ]
    },
    {
      "metadata": {
        "id": "vnbesUOudO_s",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save the initial weights of model\n",
        "init_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rj4R3qV5ABFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**L'implémentation de la fonction de coût, l'optimiseur, les boucles d'entraînement et de test du réseau reste inchangé !**"
      ]
    },
    {
      "metadata": {
        "id": "JgEcoSRQAVtA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.load_state_dict(init_model_wts)\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "num_epochs = 10\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.data[0]\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Statistics\n",
        "        valid_loss += loss.data[0]\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss/train_n_iter)\n",
        "    valid_loss_history.append(valid_loss/valid_n_iter)\n",
        "        \n",
        "    print('\\nEpoch: {}/{}'.format(epoch+1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss/train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss/valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over data.\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    if use_gpu:\n",
        "      # switch tensor type to GPU\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    # No need to flatten the images here !\n",
        "    \n",
        "    # Convert torch tensor to Variable\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    # Forward\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    correct += torch.sum(predicted == labels.data)\n",
        "\n",
        "print('\\n\\nAccuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6pxvS_yUeWog",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On obtient d'encore meilleurs résultats après 10 époques !\n",
        "\n",
        "Regardons les coubres d'entraînement et de validation :"
      ]
    },
    {
      "metadata": {
        "id": "hrv-o7Cle0ty",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save history for later\n",
        "lenet5_batchnorm_train_loss_history = train_loss_history\n",
        "lenet5_batchnorm_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, lenet5_train_loss_history, label='LeNet5 train')\n",
        "plt.plot(x, lenet5_valid_loss_history, label='LeNet5 valid')\n",
        "plt.plot(x, lenet5_batchnorm_train_loss_history, label='LeNet5 batch norm train')\n",
        "plt.plot(x, lenet5_batchnorm_valid_loss_history, label='LeNet5 batch norm valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CHqHLCP6Crmp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning : finetuning d'un réseau à convolution\n",
        "**Attribution :** cette partie reprend en partie le tutoriel : http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
        "\n",
        "En pratique, il est peu commun d'entraîner un réseau à convolution à partir de rien (c'est-à-dire avec une initialisation des poids aléatoires). En effet, souvent, le jeu de données d'intérêt est trop petit. A la place, il est commun de pré-entraîner le réseau sur un jeu de données plus gros comme, par exemple, un sous-ensemble d'ImageNet (1.2 millions d'images avec 1000 catégories). Ce réseau pré-entraîné est ensuite utilisé comme initialisation des poids du réseau qui sera entraîné sur le jeu de données d'intérêt. On parle de finetuning du réseau à convolution. A noter que le réseau pré-entraîné peut aussi être utilisé pour extraire de nouvelles variables du jeu de données d'intérêt. On parle de transfer learning.\n",
        "\n",
        "Nous allons maintenant étudier plus en détail le scénario du finetuning pour le transfer learning."
      ]
    },
    {
      "metadata": {
        "id": "itOkLFisvKqI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Télécharger les données et créer le chargeur de données\n",
        "Le jeu de données que nous allons étudier est un sous-ensemble d'ImageNet qui contient environ $120 \\times 2$ images d'entraînement et $75 \\times 2$ images de test de fourmis et d'abeilles. Le but est de classifier ces deux classes. Ci-dessous, un exemples d'images de ce jeu de données :\n",
        "\n",
        "<img src=\"https://github.com/mila-udem/ecolehiver2018/blob/master/Tutoriaux/CNN/figures/fourmi_abeille.png?raw=true\">\n",
        "\n",
        "### Boîte à outils\n",
        "**Rappel :** une façon simple de charger les données dans PyTorch est : \n",
        "<ul>\n",
        "<li>D'utiliser une classe enfant de la classe parent <a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.Dataset\">`torch.utils.data.Dataset`</a> où les méthodes `__getitem__` et `__len__` sont à compléter. Notez qu' à ce stade, les données ne sont pas chargées en mémoire.</li>\n",
        "<li>D'utiliser la classe <a href=\"http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader\">`torch.utils.data.DataLoader`</a> pour lire et mettre en mémoire les données.</li>\n",
        "</ul>\n",
        "\n",
        "**Remarque :** <a href=http://pytorch.org/docs/master/torchvision/datasets.html#torchvision-datasets>`torchvision.datasets`</a> peut aussi être utilisé pour charger des données à partir d'un dossier.\n",
        "\n",
        "**Augmentation des données :** pour augmenter les données, <a href=\"http://pytorch.org/docs/master/torchvision/transforms.html#torchvision-transforms\">`torchvision.transforms`</a> fournit les transformations d'images courantes. Ces transformations peuvent être appliquées successivement en utilisant la classe <a href=\"http://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Compose\">`torchvision.transforms.Compose`</a>.\n",
        "\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "_LW-1CauxEQM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "## DOWNLOAD DATASET ##\n",
        "if [ ! -d \"hymenoptera_data\" ]; then\n",
        "  wget --quiet https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "  unzip -q hymenoptera_data.zip\n",
        "  rm hymenoptera_data.zip\n",
        "fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OMvya9Oxps5z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "def make_dataset(root, split_type):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  root_dir : string\n",
        "    Directory with all the images.\n",
        "  split_type : string\n",
        "    The name of the split in {'train', 'valid'}.\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  images : dict\n",
        "    Dict of images path for each classes for a specific split type.\n",
        "  \"\"\"\n",
        "  \n",
        "  images = {}\n",
        "  root = os.path.join(root, split_type)\n",
        "  \n",
        "  for classes in sorted(os.listdir(root)):\n",
        "    images[classes] = []\n",
        "    path_classes = os.path.join(root, classes)\n",
        "\n",
        "    for root_, _, fnames in sorted(os.walk(path_classes)):\n",
        "      for fname in sorted(fnames):\n",
        "        if fname.endswith('.jpg'):\n",
        "          item = os.path.join(root_, fname)\n",
        "          images[classes].append(item)\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "class HymenopteraDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Hymenoptera dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, split_type='train', transform=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        root_dir : string\n",
        "           Directory with all the images.\n",
        "        split_type : string\n",
        "           The name of the split in {'train', 'valid', 'test', 'train_valid'}.\n",
        "        transform : callable, optional\n",
        "           Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split_type = split_type\n",
        "        self.transform = transform\n",
        "        self.classes = {'ants': 0, 'bees': 1}\n",
        "        \n",
        "        imgs_ = []\n",
        "        target_ = []\n",
        "        \n",
        "        if split_type == 'train':\n",
        "          imgs = make_dataset(root_dir, 'train')\n",
        "          for k, v in imgs.items():\n",
        "            imgs_ += imgs[k][:int(0.8*len(v))]\n",
        "            target_ += [self.classes[k]] * len(imgs_)\n",
        "\n",
        "        elif split_type == 'valid':\n",
        "          imgs = make_dataset(root_dir, 'train')\n",
        "          for k, v in imgs.items():\n",
        "            imgs_ += imgs[k][int(0.8*len(v)):]\n",
        "            target_ += [self.classes[k]] * len(imgs_)\n",
        "        \n",
        "        elif split_type == 'train_valid':\n",
        "          imgs = make_dataset(root_dir, 'train')\n",
        "          for k, v in imgs.items():\n",
        "            imgs_ += imgs[k]\n",
        "            target_ += [self.classes[k]] * len(imgs_)\n",
        "\n",
        "        elif split_type == 'test':\n",
        "          imgs = make_dataset(root_dir, 'val')\n",
        "          for k, v in imgs.items():\n",
        "            imgs_ += imgs[k]\n",
        "            target_ += [self.classes[k]] * len(imgs_)\n",
        "        \n",
        "        self.imgs = imgs_\n",
        "        self.target = np.array(target_)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Get the number of image in the dataset.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "           The number of images in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get the items : image, target\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        index : int\n",
        "           Index\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        img : tensor\n",
        "           The image.\n",
        "        target : int\n",
        "           Target is class_index of the target class.\n",
        "        \"\"\"\n",
        "        path = self.imgs[index]\n",
        "        target = self.target[index]\n",
        "         \n",
        "        with open(path, 'rb') as f:\n",
        "          with Image.open(f) as img:\n",
        "            img.convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "              img = self.transform(img)\n",
        "\n",
        "        return img, target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v94wZLuOp4m3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomSizedCrop(224),\n",
        "        # New version\n",
        "        # transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Scale(256),\n",
        "        # New version\n",
        "        # transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8i9eB9I_p5sJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Dataset loader\n",
        "data_dir = 'hymenoptera_data'\n",
        "\n",
        "data_train = HymenopteraDataset(data_dir, 'train', data_transforms['train'])\n",
        "train_loader = DataLoader(data_train, batch_size=4, shuffle=True, num_workers=4)\n",
        "\n",
        "data_valid = HymenopteraDataset(data_dir, 'valid', data_transforms['valid'])\n",
        "valid_loader = DataLoader(data_valid, batch_size=4, shuffle=False, num_workers=4)\n",
        "\n",
        "data_test = HymenopteraDataset(data_dir, 'test', data_transforms['valid'])\n",
        "test_loader = DataLoader(data_test, batch_size=4, shuffle=False, num_workers=4)\n",
        "\n",
        "print('# images in data train: {}'.format(len(data_train)))\n",
        "print('# images in data valid: {}'.format(len(data_valid)))\n",
        "print('# images in data test: {}'.format(len(data_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pl1X1vt5AVaf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualisons les données d'entraînement !"
      ]
    },
    {
      "metadata": {
        "id": "f4kbOO5XAUO5",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "inputs, classes = next(iter(train_loader))\n",
        "\n",
        "print('Classes: {}'.format(data_train.classes))\n",
        "print('Inputs size: {}'.format(inputs.size()))\n",
        "print('Classes size: {}'.format(classes.size()))\n",
        "\n",
        "# First image of the batch\n",
        "img1 = inputs[0]\n",
        "\n",
        "# Plot the first image\n",
        "# print('Display the first image:')\n",
        "img1 = img1.numpy().transpose((1, 2, 0))\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "img1 = std * img1 + mean\n",
        "img1 = np.clip(img1, 0, 1)\n",
        "plt.imshow(img1)\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qh3bKAO7Lf14",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "def imshow(img, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "class_names = data_train.classes\n",
        "class_names = {class_names[k]: k for k in class_names.keys()}\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gN9JbYASp4tE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Définir l'architecture du réseau\n",
        "### Boîte à outils\n",
        "Ici, nous voulons réutiliser un réseau pré-entrainé sur ImageNet. Pour cela, il faut charger un modèle pré-entraîné et réinitialiser la couche finale qui est la couche complètement connectée. Par chance, dans Pytorch, <a href=\"http://pytorch.org/docs/0.1.12/torchvision/models.html#module-torchvision.models\">`torchvision.models`</a> propose des architectures toutes faites où les poids ont déjà été entraînés sur ImageNet.\n",
        "\n",
        "Un choix commun pour un problème de classification (notre cas) est d'utiliser la classe suivante : \n",
        "<a href=\"http://pytorch.org/docs/0.1.12/torchvision/models.html#torchvision.models.resnet18\">`torchvision.models.resnet18(pretrained=True)`</a>\n",
        "\n",
        "Un exemple de bloc résiduel est donné ci-dessous :\n",
        "\n",
        "<img src=\"https://github.com/mila-udem/ecolehiver2018/blob/master/Tutoriaux/CNN/figures/bloc_residuel.png?raw=true\">\n",
        "\n",
        "**Rappel :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a> permet d'appliquer une transformation linéaire à des données d'entrée : y = Ax + b.\n",
        "\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "TiQfdQxiMJPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dans un premier temps on va utiliser un modèle non pré-entraîné puis on utilisera le même modèle mais cette fois-ci pré-entraîné."
      ]
    },
    {
      "metadata": {
        "id": "nxBgVe4NISea",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained model\n",
        "model = models.resnet18(pretrained=False)\n",
        "# model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Reset last layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "if use_gpu:\n",
        "    # switch model to GPU\n",
        "    model = model.cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQTyN1tkZyYG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save the initial weights of model\n",
        "init_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_ut6uP0qpWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Définir la fonction de coût et l'optimiseur\n",
        "### Boîte à outils\n",
        "**Rappel : ** un choix commun pour un problème de classification (notre cas) est d'utiliser les classes suivantes :\n",
        "<ul>\n",
        "<li>**Fonction de coût :** <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.CrossEntropyLoss\">`torch.nn.CrossEntropyLoss()`</a>. L'entropie croisée est souvent utilisée en optimisation. Elle permet de comparer une distribution $p$ avec une distibution de référence $t$. Elle est minimum lorsque $t=p$. Sa formule pour la calculer entre la prédiction et la cible est : $-\\sum_j t_{ij} \\log(p_{ij})$ où $p$ est la prédiction, $t$ la cible, $i$ les exemples et $j$ les classes de la cible.</li>\n",
        "<li>**Optimiseur :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a>.</li>\n",
        "</ul>\n",
        "\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "JW00K1Ssqqcm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uhoTb410Utx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entraîner le réseau\n",
        "### Boîte à outils\n",
        "**Rappel :** en général, l'entraînement d'un réseau se fait en itérant sur plusieurs époques (une époque correspond à une passe sur l'intégralité du jeu de données d'entraînement). Sur une époque on va recevoir une série de batches fournies par l'itérateur. Pour chaque batch, on fait les opérations suivantes:\n",
        "<ul>\n",
        "<li>`optimizer.zero_grad()` : on efface les gradients encore stockés par le réseau issus de la passe précédente.</li>\n",
        "<li>`loss.backward()` : on calcule automatiquement la dérivée du coût et on propage l'erreur dans le graphe par rétro-propagation.</li>\n",
        "<li>`optimizer.step()` : on effectue une étape de descente de gradient. Dans le cas de SGD, c'est une descente de gradient classique avec les gradients calculés précédemment : `poids = poids - pas_d_apprentissage * gradient`. Dans le cas d'Adam une opération légérement plus complexe est réalisée.</li>\n",
        "</ul>\n",
        "\n",
        "**Conseils bonus :** L'orsque l'on entraîne le réseau de neurones profonds, il est conseillé de faire :\n",
        "<ul>\n",
        "<li>de l'early stopping. C'est une forme de régularisation qui évite de faire du sur-apprentissage en utilisant une règle pour stopper l'apprentissage du modèle.</li>\n",
        "<li>du checkpointing. Pour cela, il est commun d'enregister les poids du réseau accessible avec `model.state_dict()` à différentes étapes de l'entraînement.</li>\n",
        "<li>d'imprimer les temps d'exécution. Pour cela, il est commun d'utilier `time.time()`.</li>\n",
        "</ul>\n",
        "\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "hXNpyIj_wxRP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "since = time.time()\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "num_epochs = 25\n",
        "best_acc = 0.0\n",
        "\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.data[0]\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "    \n",
        "        # Statistics\n",
        "        total += labels.size(0)\n",
        "        correct += torch.sum(predicted == labels.data)\n",
        "        valid_loss += loss.data[0]\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    # Deep copy the best model\n",
        "    if epoch_acc > best_acc:\n",
        "      best_acc = epoch_acc\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "print('\\n\\nBest valid accuracy: {:.2f}'.format(best_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xr8-PgSmiQSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualisons les courbes d'entraînement et de validation :"
      ]
    },
    {
      "metadata": {
        "id": "njb7oaT-hfk3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "resnet18_train_loss_history = train_loss_history\n",
        "resnet18_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, resnet18_train_loss_history, label='ResNet18 train')\n",
        "plt.plot(x, resnet18_valid_loss_history, label='ResNet18 valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vcqa0H5p-FHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tester le réseau\n",
        "### Boîte à outils\n",
        "**Rappel :** on évalue ensuite le réseau sur l'ensemble du jeu de données de test.\n",
        "\n",
        "**Remarque :** ici, nous n'avons pas de données de test donc nous testons sur l'ensemble de validation (à ne pas faire en pratique).\n",
        "\n",
        "**Utilisation des poids du meilleur modèle :** comme nous avons fait de l'early stopping lors de l'entraînement, nous voulons réutiliser les poids du meilleur modèle sur l'ensemble de validation pour tester le modèle. Ces poids ont été enregistrés lors de l'entraînement du modèle dans `best_model_wts`. Pour les charger il suffit d'utiliser `model.load_state_dict(best_model_wts)`.\n",
        "### Implémentation"
      ]
    },
    {
      "metadata": {
        "id": "C1DCCblW_EPo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test data\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    if use_gpu:\n",
        "      # switch tensor type to GPU\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    # Convert torch tensor to Variable\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    # Forward\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    correct += torch.sum(predicted == labels.data)\n",
        "\n",
        "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d38Q32VMN0Hb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Avec les poids pré-entraînés :"
      ]
    },
    {
      "metadata": {
        "id": "qMbvwaR5Ny-K",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "# Load pre-trained model\n",
        "# model = models.resnet18(pretrained=False)\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Reset last layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "if use_gpu:\n",
        "    # switch model to GPU\n",
        "    model = model.cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "print(\"\\n\\n# Parameters: \", sum([param.nelement() for param in model.parameters()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XAcPL09nN-Ki",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Save the initial weights of model\n",
        "init_model_wts = copy.deepcopy(model.state_dict())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88Nhc4mlN-sc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U1pQBFbAOEjd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "since = time.time()\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "num_epochs = 25\n",
        "best_acc = 0.0\n",
        "\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "print(\"# Start training #\")\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "    \n",
        "    # Iterate over train data\n",
        "    for images, labels in train_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # Zero the gradient buffer\n",
        "        optimizer.zero_grad()  \n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimize\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        train_loss += loss.data[0]\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Set model to evaluate mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Iterate over valid data\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in valid_loader:  \n",
        "        \n",
        "        if use_gpu:\n",
        "          # switch tensor type to GPU\n",
        "          images = images.cuda()\n",
        "          labels = labels.cuda()\n",
        "        \n",
        "        # Convert torch tensor to Variable\n",
        "        images = Variable(images)\n",
        "        labels = Variable(labels)\n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "    \n",
        "        # Statistics\n",
        "        total += labels.size(0)\n",
        "        correct += torch.sum(predicted == labels.data)\n",
        "        valid_loss += loss.data[0]\n",
        "        valid_n_iter += 1\n",
        "    \n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    # Deep copy the best model\n",
        "    if epoch_acc > best_acc:\n",
        "      best_acc = epoch_acc\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print('\\nEpoch: {}/{}'.format(epoch + 1, num_epochs))\n",
        "    print('\\tTrain Loss: {:.4f}'.format(train_loss / train_n_iter))\n",
        "    print('\\tValid Loss: {:.4f}'.format(valid_loss / valid_n_iter))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nTraining complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "print('\\n\\nBest valid accuracy: {:.2f}'.format(best_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_x_Kot7iEnI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualisons les courbes d'entraînement et de validation :"
      ]
    },
    {
      "metadata": {
        "id": "sVUPxQCehyzc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "resnet18_pretrained_train_loss_history = train_loss_history\n",
        "resnet18_pretrained_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Plot training and validation curve\n",
        "x = range(1, num_epochs + 1)\n",
        "plt.plot(x, resnet18_train_loss_history, label='ResNet18 train')\n",
        "plt.plot(x, resnet18_valid_loss_history, label='ResNet18 valid')\n",
        "plt.plot(\n",
        "      x, resnet18_pretrained_train_loss_history,\n",
        "    label='ResNet18 pretrained train')\n",
        "plt.plot(\n",
        "      x, resnet18_pretrained_valid_loss_history,\n",
        "    label='ResNet18 pretrained valid')\n",
        "\n",
        "plt.xlabel('# epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "keq8hL3yiCOG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Testons le modèle :"
      ]
    },
    {
      "metadata": {
        "id": "MJy4FdfQfc7z",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load best model weights\n",
        "model.load_state_dict(best_model_wts)\n",
        "\n",
        "# Set model to evaluate mode\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Iterate over test data\n",
        "for images, labels in test_loader:\n",
        "    \n",
        "    if use_gpu:\n",
        "      # switch tensor type to GPU\n",
        "      images = images.cuda()\n",
        "      labels = labels.cuda()\n",
        "    \n",
        "    # Convert torch tensor to Variable\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    # Forward\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    \n",
        "    # Statistics\n",
        "    total += labels.size(0)\n",
        "    correct += torch.sum(predicted == labels.data)\n",
        "\n",
        "print('Accuracy on the test set: {:.2f}%'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LMd3mx5KKYXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On observe une augmentation de l'accuracy sur le test par rapport au modèle dont les poids n'avaient pas été entraîné."
      ]
    },
    {
      "metadata": {
        "id": "IsLwCjnbWkk4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Références\n",
        "Certaines parties de ce tutoriel sont fortement inspirées des tutoriaux suivant :\n",
        "<ul>\n",
        "<li>https://github.com/andrewliao11/dni.pytorch/blob/master/mlp.py</li>\n",
        "<li>https://github.com/andrewliao11/dni.pytorch/blob/master/cnn.py</li>\n",
        "<li>http://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html</li>\n",
        "<li>http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html</li>\n",
        "</ul>"
      ]
    }
  ]
}